"""
YOLO26-DINO-MLP FEN Detector

Combines YOLO26 for piece detection with DINO-MLP for classification to achieve 99%+ FEN accuracy.

YOLO26 advantages (released Jan 14, 2026):
- 43% faster CPU inference than YOLO11
- End-to-end NMS-free architecture (simpler deployment)
- Better small object detection with ProgLoss + STAL
- Perfect for chess pieces!

Architecture:
1. YOLO26 detects piece bounding boxes (fast, accurate)
2. DINO extracts features from each cropped piece (pretrained ViT)
3. MLP classifies each piece (12 classes, 99%+ accuracy)
4. Generate FEN from piece positions

Usage:
    from cosmos_chessbot.vision.yolo_dino_detector import YOLODINOFenDetector

    detector = YOLODINOFenDetector(
        yolo_weights='runs/detect/yolo26_chess/weights/best.pt',
        mlp_weights='models/dino_mlp/dino_mlp_best.pth'
    )

    fen = detector.detect_fen(image)  # Returns FEN string
"""

import cv2
import torch
import torch.nn as nn
from pathlib import Path
from typing import Optional, List, Tuple
import numpy as np
from PIL import Image
import chess

try:
    from ultralytics import YOLO
    from transformers import ViTImageProcessor, ViTModel
except ImportError as e:
    print(f"Missing dependency: {e}")
    print("Install with: pip3 install ultralytics transformers pillow torch")
    raise


# Class mapping -- must match data/chessred2k_yolo/chess_dataset.yaml:
#   0: white-pawn, 1: white-rook, 2: white-knight, 3: white-bishop, 4: white-queen, 5: white-king
#   6: black-pawn, 7: black-rook, 8: black-knight, 9: black-bishop, 10: black-queen, 11: black-king
CLASS_TO_PIECE = {
    0: 'P', 1: 'R', 2: 'N', 3: 'B', 4: 'Q', 5: 'K',  # White
    6: 'p', 7: 'r', 8: 'n', 9: 'b', 10: 'q', 11: 'k',  # Black
}

CLASS_NAMES = [
    'white_pawn', 'white_rook', 'white_knight', 'white_bishop', 'white_queen', 'white_king',
    'black_pawn', 'black_rook', 'black_knight', 'black_bishop', 'black_queen', 'black_king'
]


class DINOChessPieceClassifier(nn.Module):
    """DINO + MLP classifier for chess pieces."""

    def __init__(self, num_classes=12):
        super().__init__()

        # Pretrained DINO ViT-S/8
        self.dino = ViTModel.from_pretrained('facebook/dino-vits8')
        self.dino_feature_dim = self.dino.config.hidden_size  # 384

        # Freeze DINO
        for param in self.dino.parameters():
            param.requires_grad = False

        # 3-layer MLP
        self.classifier = nn.Sequential(
            nn.Linear(self.dino_feature_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes)
        )

    def forward(self, pixel_values):
        with torch.no_grad():
            outputs = self.dino(pixel_values=pixel_values)
            features = outputs.last_hidden_state[:, 0]  # CLS token
        return self.classifier(features)


class YOLODINOFenDetector:
    """YOLO26 + DINO-MLP FEN detector."""

    def __init__(
        self,
        yolo_weights: str,
        corner_weights: Optional[str] = None,
        mlp_weights: Optional[str] = None,
        device: str = 'mps',
        conf_threshold: float = 0.10,
        use_dino: bool = True,
        static_corners: Optional[str] = None,
    ):
        """
        Initialize YOLO-DINO FEN detector.

        Args:
            yolo_weights: Path to trained YOLO26 piece detection weights
            corner_weights: Path to trained YOLO26 pose corner detection weights
                (required unless static_corners is provided)
            mlp_weights: Path to trained DINO-MLP weights (if None, uses YOLO classes)
            device: Device to use ('mps', 'cuda', or 'cpu')
            conf_threshold: Confidence threshold for YOLO detections (default: 0.10)
                Lower threshold (0.10) recovers 77% more pieces vs 0.25 with minimal
                false positives, achieving 99.73% accuracy and 85.2% exact FEN matches.
            use_dino: Whether to use DINO-MLP for re-classification
            static_corners: Path to JSON file with calibrated corners (skips pose model)
                Format: {"corners": [[x0,y0],[x1,y1],[x2,y2],[x3,y3]],
                         "resolution": [width, height]}
        """
        self.device = device
        self.conf_threshold = conf_threshold
        self.use_dino = use_dino and mlp_weights is not None

        # Load YOLO piece detection model
        print(f"Loading piece detection model from {yolo_weights}")
        self.yolo = YOLO(yolo_weights)

        # Static calibrated corners (if provided, skip pose model entirely)
        self._static_corners: Optional[np.ndarray] = None
        self._static_resolution: Optional[Tuple[int, int]] = None
        if static_corners is not None:
            import json
            with open(static_corners) as f:
                cal = json.load(f)
            self._static_corners = np.array(cal['corners'], dtype=np.float32)
            self._static_resolution = tuple(cal['resolution'])  # (width, height)
            print(f"Loaded calibrated corners from {static_corners} "
                  f"(resolution {self._static_resolution})")
            self.corner_model = None
        else:
            if corner_weights is None:
                raise ValueError("Either corner_weights or static_corners must be provided")
            print(f"Loading corner detection model from {corner_weights}")
            self.corner_model = YOLO(corner_weights)

        # Cached homography matrix (recomputed when corners change)
        self._homography: Optional[np.ndarray] = None

        # Load DINO-MLP if provided
        if self.use_dino:
            print(f"Loading DINO-MLP model from {mlp_weights}")
            self.mlp = DINOChessPieceClassifier(num_classes=12)
            self.mlp.load_state_dict(torch.load(mlp_weights, map_location=device))
            self.mlp = self.mlp.to(device)
            self.mlp.eval()

            # DINO preprocessor
            self.dino_processor = ViTImageProcessor.from_pretrained('facebook/dino-vits8')
        else:
            self.mlp = None
            self.dino_processor = None

        print(f"Detector initialized (use_dino={self.use_dino})")

    def _detect_corners(self, image: np.ndarray) -> Optional[np.ndarray]:
        """
        Detect board corners using the YOLO26 pose model, or return
        calibrated static corners (scaled to the current image resolution).

        Returns corners as a (4, 2) array in keypoint order:
            0: top_left, 1: top_right, 2: bottom_right, 3: bottom_left

        Args:
            image: RGB image as numpy array

        Returns:
            (4, 2) float32 array of corner pixel coordinates, or None if not detected
        """
        if self._static_corners is not None:
            # Scale static corners if image resolution differs from calibration
            img_h, img_w = image.shape[:2]
            cal_w, cal_h = self._static_resolution
            corners = self._static_corners.copy()
            if img_w != cal_w or img_h != cal_h:
                corners[:, 0] *= img_w / cal_w
                corners[:, 1] *= img_h / cal_h
            return corners

        results = self.corner_model.predict(image, verbose=False)
        if len(results) == 0 or results[0].keypoints is None:
            return None

        kpts = results[0].keypoints
        if len(kpts.xy) == 0:
            return None

        # Take the highest-confidence detection
        corners = kpts.xy[0].cpu().numpy()  # shape (4, 2)
        return corners.astype(np.float32)

    def _compute_homography(self, corners: np.ndarray) -> np.ndarray:
        """
        Compute homography matrix that maps board pixel coordinates
        to a canonical 8x8 grid where (0,0) is top_left and (8,8) is bottom_right.

        Args:
            corners: (4, 2) array [top_left, top_right, bottom_right, bottom_left]

        Returns:
            3x3 homography matrix
        """
        # Source: detected corners in keypoint order (TL, TR, BR, BL)
        src = corners.reshape(4, 1, 2)

        # Destination: canonical 8x8 board corners
        dst = np.array([
            [0, 0],   # top_left
            [8, 0],   # top_right
            [8, 8],   # bottom_right
            [0, 8],   # bottom_left
        ], dtype=np.float32).reshape(4, 1, 2)

        H, _ = cv2.findHomography(src, dst)
        return H

    def _pixel_to_square(
        self,
        x_center: float,
        y_center: float,
        homography: np.ndarray
    ) -> Optional[chess.Square]:
        """
        Convert pixel coordinates to chess square using homography.

        The homography maps image pixels into canonical board coordinates
        where x in [0,8] is the file (a-h) and y in [0,8] is the rank (8->1,
        i.e. visual top = rank 8).

        Args:
            x_center: X coordinate in pixels
            y_center: Y coordinate in pixels
            homography: 3x3 homography matrix from _compute_homography

        Returns:
            chess.Square or None if the point falls outside the board
        """
        # Apply homography: transform pixel point to board coordinates
        pt = np.array([[[x_center, y_center]]], dtype=np.float32)
        mapped = cv2.perspectiveTransform(pt, homography)[0, 0]
        bx, by = float(mapped[0]), float(mapped[1])

        # Check if within board bounds [0, 8]
        if not (0 <= bx <= 8 and 0 <= by <= 8):
            return None

        file_idx = int(bx)
        rank_idx = int(by)

        # Clamp to valid range [0, 7]
        file_idx = max(0, min(7, file_idx))
        rank_idx = max(0, min(7, rank_idx))

        # Visual top (by=0) is rank 8 (index 7), bottom is rank 1 (index 0)
        rank_idx = 7 - rank_idx

        try:
            return chess.square(file_idx, rank_idx)
        except ValueError:
            return None

    def detect_fen(
        self,
        image: np.ndarray,
        verbose: bool = False
    ) -> str:
        """
        Detect FEN position from image.

        Args:
            image: Input image (BGR or RGB numpy array)
            verbose: Print debug information

        Returns:
            FEN string (board position only, no turn/castling info)
        """
        # Handle BGR to RGB conversion
        if len(image.shape) == 3 and image.shape[2] == 3:
            # Assume BGR (OpenCV format), convert to RGB
            image_rgb = image[:, :, ::-1]
        else:
            image_rgb = image

        img_height, img_width = image_rgb.shape[:2]

        # Step 1: Detect board corners and compute homography
        corners = self._detect_corners(image_rgb)
        if corners is None:
            if verbose:
                print("Board corners not detected")
            return "8/8/8/8/8/8/8/8"

        homography = self._compute_homography(corners)
        if verbose:
            print(f"Corners detected: {corners.tolist()}")

        # Step 2: Detect pieces with YOLO
        results = self.yolo.predict(image_rgb, conf=self.conf_threshold, verbose=False)

        if len(results) == 0 or len(results[0].boxes) == 0:
            if verbose:
                print("No pieces detected")
            return "8/8/8/8/8/8/8/8"  # Empty board

        detections = results[0].boxes

        # Create empty board
        board = chess.Board(fen=None)
        board.clear()

        piece_count = 0

        # Process each detection
        for i, box in enumerate(detections):
            # Get bounding box
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            x_center = (x1 + x2) / 2
            # Use 90% down from top (10% up from bottom) - where piece base is
            bbox_height = y2 - y1
            y_point = y1 + 0.9 * bbox_height

            # Get YOLO class
            yolo_class = int(box.cls[0])
            yolo_conf = float(box.conf[0])

            # Map to chess square via homography
            square = self._pixel_to_square(x_center, y_point, homography)
            if square is None:
                if verbose:
                    print(f"Detection {i} out of board bounds")
                continue

            # Step 2: Re-classify with DINO-MLP (if enabled)
            if self.use_dino:
                # Crop piece
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                x1 = max(0, x1)
                y1 = max(0, y1)
                x2 = min(img_width, x2)
                y2 = min(img_height, y2)

                if x2 <= x1 or y2 <= y1:
                    continue

                piece_crop = Image.fromarray(image_rgb[y1:y2, x1:x2])

                # Extract DINO features and classify
                inputs = self.dino_processor(images=piece_crop, return_tensors='pt')
                pixel_values = inputs['pixel_values'].to(self.device)

                with torch.no_grad():
                    logits = self.mlp(pixel_values)
                    dino_class = int(torch.argmax(logits, dim=1)[0])
                    dino_conf = float(torch.softmax(logits, dim=1)[0, dino_class])

                # Use DINO classification
                final_class = dino_class
                final_conf = dino_conf

                if verbose:
                    print(f"Detection {i}: YOLO={CLASS_NAMES[yolo_class]}({yolo_conf:.2f}), "
                          f"DINO={CLASS_NAMES[dino_class]}({dino_conf:.2f}), "
                          f"Square={chess.square_name(square)}")
            else:
                # Use YOLO classification
                final_class = yolo_class
                final_conf = yolo_conf

                if verbose:
                    print(f"Detection {i}: YOLO={CLASS_NAMES[yolo_class]}({yolo_conf:.2f}), "
                          f"Square={chess.square_name(square)}")

            # Convert class to piece
            piece_symbol = CLASS_TO_PIECE[final_class]
            piece = chess.Piece.from_symbol(piece_symbol)

            # Place piece on board (if square not already occupied)
            if board.piece_at(square) is None:
                board.set_piece_at(square, piece)
                piece_count += 1
            elif verbose:
                print(f"Warning: Square {chess.square_name(square)} already occupied")

        if verbose:
            print(f"\nTotal pieces placed: {piece_count}")

        # Post-processing: enforce exactly one king per side.
        # YOLO frequently confuses kings with rooks/knights due to similar
        # tall silhouettes.  When a king is missing we find the tallest
        # non-pawn piece of that colour and reclassify it.
        board = self._enforce_kings(board, detections, homography, verbose)

        # Return FEN (board position only)
        fen = board.board_fen()
        return fen

    def _enforce_kings(
        self,
        board: chess.Board,
        detections,
        homography: np.ndarray,
        verbose: bool = False,
    ) -> chess.Board:
        """Ensure exactly one white king and one black king exist on the board.

        If a king is missing for a colour, the tallest non-pawn piece of that
        colour is reclassified as the king.  This compensates for YOLO
        frequently confusing kings (tallest piece) with rooks or knights.

        Args:
            board: Board with pieces placed from YOLO detections.
            detections: Raw YOLO detection boxes (results[0].boxes).
            homography: 3x3 homography matrix.
            verbose: Print debug info.

        Returns:
            Board (mutated in-place) with exactly one king per side.
        """
        has_white_king = any(
            board.piece_at(sq) == chess.Piece(chess.KING, chess.WHITE)
            for sq in chess.SQUARES
        )
        has_black_king = any(
            board.piece_at(sq) == chess.Piece(chess.KING, chess.BLACK)
            for sq in chess.SQUARES
        )

        if has_white_king and has_black_king:
            return board

        # Build a mapping: square -> (bbox_height, yolo_class) for reclassification
        square_to_height: dict[chess.Square, float] = {}
        for box in detections:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            bbox_height = float(y2 - y1)
            x_center = (x1 + x2) / 2
            y_point = y1 + 0.9 * bbox_height

            sq = self._pixel_to_square(x_center, y_point, homography)
            if sq is not None and sq not in square_to_height:
                square_to_height[sq] = bbox_height

        for color, king_piece, is_white, missing_label in [
            (chess.WHITE, chess.Piece(chess.KING, chess.WHITE), True, "White"),
            (chess.BLACK, chess.Piece(chess.KING, chess.BLACK), False, "Black"),
        ]:
            has_king = has_white_king if is_white else has_black_king
            if has_king:
                continue

            # Find the tallest non-pawn piece of this colour
            best_sq = None
            best_h = -1.0
            for sq in chess.SQUARES:
                piece = board.piece_at(sq)
                if piece is None or piece.color != color:
                    continue
                if piece.piece_type == chess.PAWN:
                    continue
                h = square_to_height.get(sq, 0.0)
                if h > best_h:
                    best_h = h
                    best_sq = sq

            if best_sq is not None:
                old_piece = board.piece_at(best_sq)
                board.set_piece_at(best_sq, king_piece)
                if verbose:
                    print(
                        f"King enforcement: {missing_label} king missing â€” "
                        f"reclassified {old_piece.symbol()} on "
                        f"{chess.square_name(best_sq)} (bbox_h={best_h:.0f}px) as "
                        f"{king_piece.symbol()}"
                    )
            else:
                if verbose:
                    print(
                        f"King enforcement: {missing_label} king missing and "
                        f"no candidate piece found"
                    )

        return board

    def detect_fen_with_metadata(
        self,
        image: np.ndarray
    ) -> dict:
        """
        Detect FEN with additional metadata.

        Args:
            image: Input image

        Returns:
            Dict with 'fen', 'pieces' list, and detection confidence
        """
        # Handle BGR to RGB
        if len(image.shape) == 3 and image.shape[2] == 3:
            image_rgb = image[:, :, ::-1]
        else:
            image_rgb = image

        img_height, img_width = image_rgb.shape[:2]

        # Detect board corners and compute homography
        corners = self._detect_corners(image_rgb)
        if corners is None:
            return {
                'fen': "8/8/8/8/8/8/8/8",
                'pieces': [],
                'confidence': 0.0
            }

        homography = self._compute_homography(corners)

        # Detect pieces with YOLO
        results = self.yolo.predict(image_rgb, conf=self.conf_threshold, verbose=False)

        if len(results) == 0 or len(results[0].boxes) == 0:
            return {
                'fen': "8/8/8/8/8/8/8/8",
                'pieces': [],
                'confidence': 0.0
            }

        detections = results[0].boxes
        board = chess.Board(fen=None)
        board.clear()

        pieces_metadata = []
        confidences = []

        for box in detections:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            x_center = (x1 + x2) / 2
            # Use 90% down from top (10% up from bottom)
            bbox_height = y2 - y1
            y_point = y1 + 0.9 * bbox_height

            yolo_class = int(box.cls[0])
            yolo_conf = float(box.conf[0])

            square = self._pixel_to_square(x_center, y_point, homography)
            if square is None:
                continue

            # Re-classify with DINO if enabled
            if self.use_dino:
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(img_width, x2), min(img_height, y2)

                if x2 <= x1 or y2 <= y1:
                    continue

                piece_crop = Image.fromarray(image_rgb[y1:y2, x1:x2])
                inputs = self.dino_processor(images=piece_crop, return_tensors='pt')
                pixel_values = inputs['pixel_values'].to(self.device)

                with torch.no_grad():
                    logits = self.mlp(pixel_values)
                    dino_class = int(torch.argmax(logits, dim=1)[0])
                    dino_conf = float(torch.softmax(logits, dim=1)[0, dino_class])

                final_class = dino_class
                final_conf = dino_conf
            else:
                final_class = yolo_class
                final_conf = yolo_conf

            piece_symbol = CLASS_TO_PIECE[final_class]
            piece = chess.Piece.from_symbol(piece_symbol)

            if board.piece_at(square) is None:
                board.set_piece_at(square, piece)
                pieces_metadata.append({
                    'square': chess.square_name(square),
                    'piece': piece_symbol,
                    'class': CLASS_NAMES[final_class],
                    'confidence': final_conf,
                    'bbox': [float(x1), float(y1), float(x2), float(y2)]
                })
                confidences.append(final_conf)

        return {
            'fen': board.board_fen(),
            'pieces': pieces_metadata,
            'confidence': np.mean(confidences) if confidences else 0.0
        }
