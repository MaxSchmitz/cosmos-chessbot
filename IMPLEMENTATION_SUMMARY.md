# Dual-Policy Implementation Summary

Implementation completed on 2026-01-30

## What Was Built

A dual-policy chess manipulation system that supports both **Ï€â‚€.â‚…** (LeRobot) and **Cosmos Policy**, enabling empirical comparison on the same task.

## Files Created

### 1. Core Policy Infrastructure

**`src/cosmos_chessbot/policy/base_policy.py`**
- Abstract `BasePolicy` interface
- `PolicyAction` dataclass for unified action representation
- Methods: `reset()`, `select_action()`, `plan_action()`

**`src/cosmos_chessbot/policy/pi05_policy.py`**
- Ï€â‚€.â‚… implementation using LeRobot
- Language-conditioned action prediction
- Loads pretrained or fine-tuned models
- Returns single action (no planning)

**`src/cosmos_chessbot/policy/cosmos_policy.py`**
- Cosmos Policy implementation with world model
- Multi-candidate action generation
- Future state prediction
- Value-based action ranking

### 2. Training Scripts

**`scripts/train_pi05.py`**
- Fine-tune Ï€â‚€.â‚… on collected data
- Wraps `lerobot-train` command
- Configurable: steps, batch size, learning rate
- Default: 50,000 steps

**`scripts/train_cosmos.py`**
- Train Cosmos Policy on same dataset
- Uses NVIDIA Cosmos training recipe
- Configurable: steps, batch size, horizon
- Default: 30,000 steps

**`scripts/compare_policies.py`**
- Run both policies on test scenarios
- Measure: success rate, execution time, failures
- Generate comparison table and JSON output
- Default: 50 test episodes per policy

### 3. Orchestrator Updates

**`src/cosmos_chessbot/orchestrator/orchestrator.py`**

Added:
- `policy_type` config field ("pi05" or "cosmos")
- `policy_checkpoint` config field
- `enable_planning` config field
- `_init_policy()` method for policy selection
- Updated `execute()` to use selected policy
- Placeholder `_get_robot_state()` and `_execute_robot_action()`

**`src/cosmos_chessbot/orchestrator/__init__.py`**
- Export `OrchestratorConfig` for imports

### 4. CLI Updates

**`src/cosmos_chessbot/main.py`**

Added arguments:
- `--policy {pi05,cosmos}` - Select policy
- `--policy-checkpoint PATH` - Checkpoint path
- `--enable-planning` - Enable Cosmos planning
- `--no-enable-planning` - Disable Cosmos planning

### 5. Documentation

**`DUAL_POLICY_GUIDE.md`**
- Complete usage guide
- Training instructions
- CLI reference
- Troubleshooting
- Development roadmap

**`IMPLEMENTATION_SUMMARY.md`** (this file)
- What was built
- Files created
- Architecture overview
- Next steps

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Collection (Once)                    â”‚
â”‚                  LeRobot Teleoperation                       â”‚
â”‚              data/episodes/chess-manipulation/               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train Ï€â‚€.â‚…      â”‚            â”‚  Train Cosmos    â”‚
â”‚  (LeRobot)       â”‚            â”‚  Policy          â”‚
â”‚                  â”‚            â”‚                  â”‚
â”‚  50k steps       â”‚            â”‚  30k steps       â”‚
â”‚  Language-cond   â”‚            â”‚  Planning        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
         â””â”€â”€â”€â–¶â”‚  BasePolicy       â”‚â—€â”€â”€â”€â”€â”€â”˜
              â”‚  Interface        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestrator    â”‚          â”‚  Orchestrator    â”‚
â”‚  --policy pi05   â”‚          â”‚  --policy cosmos â”‚
â”‚                  â”‚          â”‚  --enable-planningâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Comparison      â”‚
              â”‚  Metrics         â”‚
              â”‚  compare_policies.py â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### 1. Unified Policy Interface

All policies implement `BasePolicy`:
- `select_action()` - Get single action
- `plan_action()` - Get multiple candidates (planning)
- `reset()` - Reset episode state

Benefits:
- Easy policy swapping
- Fair comparison
- Extensible to new policies

### 2. Policy Selection at Runtime

```bash
# Run with Ï€â‚€.â‚…
cosmos-chessbot --policy pi05 --policy-checkpoint checkpoints/pi05_chess/final.pt

# Run with Cosmos (direct)
cosmos-chessbot --policy cosmos --no-enable-planning

# Run with Cosmos (planning)
cosmos-chessbot --policy cosmos --enable-planning
```

### 3. Planning Capability

Cosmos Policy can generate multiple action candidates:
- Sample diverse actions
- Predict future states
- Rank by success probability
- Select best action

### 4. Comprehensive Comparison

`compare_policies.py` evaluates:
- Success rate (% correct moves)
- Execution time (seconds per move)
- Failure modes (dropped pieces, wrong placement)
- Planning benefit (Cosmos direct vs planning)

## Testing

All imports verified:
```bash
âœ“ base_policy imports OK
âœ“ pi05_policy imports OK
âœ“ cosmos_policy imports OK
âœ“ CLI arguments working
```

CLI help shows new options:
```
--policy {pi05,cosmos}        Policy to use (default: cosmos)
--policy-checkpoint PATH      Path to checkpoint
--enable-planning             Enable planning for Cosmos
--no-enable-planning          Disable planning
```

## Status

### âœ… Completed (This Implementation)

1. Abstract policy interface
2. Ï€â‚€.â‚… policy wrapper
3. Cosmos Policy wrapper
4. Orchestrator policy selection
5. CLI argument handling
6. Training scripts (both policies)
7. Comparison evaluation script
8. Documentation

### ğŸš§ Next Steps (Not Yet Implemented)

1. **Data Collection** (Week 1)
   - Set up SO-101 robot with LeRobot
   - Collect 100-200 teleoperation episodes
   - Verify data quality

2. **Robot Integration** (Week 1-2)
   - Implement `_get_robot_state()` to read SO-101 state
   - Implement `_execute_robot_action()` to control SO-101
   - Test on real hardware

3. **Policy Training** (Week 2-3)
   - Train Ï€â‚€.â‚… on collected data
   - Train Cosmos Policy on same data
   - Monitor convergence

4. **Evaluation** (Week 3-4)
   - Run `compare_policies.py` on test set
   - Analyze failure modes
   - Choose best policy for demo

5. **Demo Preparation** (Week 4)
   - Polish best-performing policy
   - Record submission video
   - Document results

## Usage Examples

### Training

```bash
# Train Ï€â‚€.â‚… (after data collection)
python scripts/train_pi05.py --steps 50000 --batch-size 4

# Train Cosmos Policy
python scripts/train_cosmos.py --steps 30000 --batch-size 4
```

### Inference

```bash
# Ï€â‚€.â‚… with language instructions
cosmos-chessbot \
  --policy pi05 \
  --policy-checkpoint checkpoints/pi05_chess/final.pt \
  --moves 10

# Cosmos with planning
cosmos-chessbot \
  --policy cosmos \
  --policy-checkpoint checkpoints/cosmos_chess/final.pt \
  --enable-planning \
  --moves 10
```

### Comparison

```bash
# Compare all policies
python scripts/compare_policies.py --num-episodes 50

# Results saved to data/eval/policy_comparison.json
```

## Dependencies

### Already Installed
- `torch` (2.7.0) - PyTorch
- `transformers` - Hugging Face models
- `pillow` - Image processing
- `numpy` - Arrays

### To Install (Separate Environments)

**For Ï€â‚€.â‚…:**
```bash
# In lerobot_env
pip install lerobot gymnasium opencv-python
```

**For Cosmos Policy:**
```bash
# In main env or cosmos_env
# Install from NVIDIA when available
git clone https://github.com/NVIDIA/Cosmos.git
cd Cosmos && pip install -e .
```

## Design Decisions

### 1. Why Separate Policy Interface?

- **Comparison fairness**: Both policies use same orchestrator
- **Extensibility**: Easy to add new policies
- **Debugging**: Isolate policy vs system issues

### 2. Why Both Cosmos Direct and Planning?

To measure planning benefit:
- Direct: Baseline performance
- Planning: Value of world model

Expected: Planning helps with occlusions, captures, recovery

### 3. Why Same Dataset for Both?

Eliminates data quality as confounding variable:
- Fair comparison
- Same task distribution
- Same level of diversity

## Verification

Test that implementation works:

```bash
# Test imports
uv run python -c "from src.cosmos_chessbot.policy.base_policy import BasePolicy"

# Test CLI
uv run cosmos-chessbot --help | grep policy

# Test orchestrator import
uv run python -c "from cosmos_chessbot.orchestrator import OrchestratorConfig; print('OK')"
```

All tests passing âœ“

## References

- Plan document (see conversation history)
- [LeRobot](https://github.com/huggingface/lerobot)
- [NVIDIA Cosmos](https://github.com/NVIDIA/Cosmos)
- Ï€â‚€.â‚… paper: [Link when available]
- Cosmos Policy paper: [Link when available]

## Notes

- Cosmos Policy integration uses placeholders until package is installed
- Robot control methods (`_get_robot_state`, `_execute_robot_action`) need implementation
- Data collection script (`collect_episodes.py`) may need updates for SO-101
- All training/evaluation assumes H100 GPU (can adjust batch size for other GPUs)

## Contact

For questions or issues, see:
- `DUAL_POLICY_GUIDE.md` - Usage instructions
- `README.md` - Main project README
- GitHub issues - Bug reports
